{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import porter as p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Données pour test basique*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[\"the new home has been saled on top forecasts\",\n",
    "     \"the home sales rise in july\",\n",
    "     \"there is an increase in home sales in july\",\n",
    "     \"july encounter a new home sales rise\"]\n",
    "\n",
    "stopWords=[\"the\",\"a\",\"an\",\"on\",\"behind\",\"under\",\"there\",\"in\"]\n",
    "\n",
    "index = {}\n",
    "for i in range(len(docs)):\n",
    "    index[i] = dict(Counter(map(p.stem, [word for word in (str.lower(docs[i])).split() if word not in stopWords])))           \n",
    "            \n",
    "indexInverse = {}\n",
    "for numDoc, dico in index.items():\n",
    "    for word, tf in dico.items():\n",
    "        if(word not in indexInverse):\n",
    "            indexInverse[word]= {}\n",
    "        indexInverse[word][numDoc] = tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'optimiser le calcul du score, on utilisera l'index inversé qui permet de retrouver les documents à partir des termes (et donc à partir de la requete) plutôt que d'utiliser l'index qui nous oblige à parcourir tous les documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On considère ici un modèle booleen simple où une requete du type \"mot1 mot2 mot3\" équivaut à \"mot1^mot2^mot3\" i.e un document contenant tous les mots de la requete. <br/>\n",
    "Dans cette situation, on peut simplement récupérer les documents contenants le i-ème mot dans un l'ensemble i et prendre l'intersection de tous ces ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Modele booleen\n",
    "req_str=\"home sales top top\" #requete avec format \"humain\"\n",
    "#découpe la requete, applique une stemmatisation et supprime les doublons (car modèle booleen)\n",
    "req = list(np.unique(list(map(p.stem, req_str.split())))) \n",
    "#représentation du résultat comme unensemble de documents\n",
    "res=set(index)\n",
    "for stem in req:\n",
    "    res=res.intersection(indexInverse[stem])#On récupère l'intersection des documents contenant un mot de la requete\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation : <br/>\n",
    "- $w_{t,d}$ est le poids d'un terme $t$ dans un document $d$ et $w_{t,q}$ le poids d'un terme $t$ dans une requete $q$.<br/>\n",
    "- $tf_{t,d}$ (resp. $tf_{t,q}$) correspond au *term frequency* du terme $t$  dans le document $d$ (resp. la requete $q$).<br/>\n",
    "- $idf_t$ correspond à l'*inverse document frequency* du terme $t$ dans l'ensemble de la collection/du corpus considéré<br/><br/>\n",
    "\n",
    "La méthode **getWeightsForDoc** (resp. **getWeightsForStem**) permet de récupérer tous les poids $w_{t,d}$ pour un document $d$ donné (resp. un terme $t$ donné).<br/>\n",
    "La méthode **getWeightsForQuery** permet de récupérer tous les poids $w_{t,q}$ pour une requete $q$ donnée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighter():\n",
    "    \"\"\"\n",
    "    Classe abstraite représentant le squelette d'une pondération.\n",
    "    \"\"\"\n",
    "    def __init__(self, index, indexInverse):\n",
    "        \"\"\"\n",
    "        Permet d'initaliser en indiquant l'index et l'index inversé utilisé\n",
    "        \n",
    "        :param index: L'index considéré\n",
    "        :param indexInverse: L'index inversé considéré\n",
    "\n",
    "        :type index: dict[int : dict[string: int]]\n",
    "        :type indexInverse: dict[string : dict[int: int]]\n",
    "        \"\"\"\n",
    "        self.index = index\n",
    "        self.indexInverse = indexInverse\n",
    "        self.idf = {}\n",
    "        self.norm = {}\n",
    "        \n",
    "    def getIdf(self, stem):\n",
    "        \"\"\"\n",
    "        Permet de récupérer l'idf (index inverse frequency) d'une terme dans le corpus considéré.\n",
    "        \n",
    "        :param stem: Le terme dont l'idf doit être calculé\n",
    "\n",
    "        :type stem: string\n",
    "        \n",
    "        :return: L'idf\n",
    "        :rtype: float \n",
    "        \"\"\"\n",
    "        #Si l'idf du terme n'a jamais été calculé, on le calul et on l'enregistre avant de renvoyer la valeur\n",
    "        if(stem not in self.idf):\n",
    "            if(stem not in indexInverse):\n",
    "                df = 0\n",
    "            else:\n",
    "                df = len(indexInverse[stem])\n",
    "            self.idf[stem] = np.log((1+len(self.index)) / (1+df))\n",
    "        return self.idf[stem]\n",
    "    \n",
    "    def getWeightsForDoc(self, idDoc):\n",
    "        \"\"\"\n",
    "        Permet de récupérer les poids des termes du document indiqué.\n",
    "        \n",
    "        :param idDoc: L'id du document à considérer\n",
    "\n",
    "        :type idDoc: int\n",
    "        \n",
    "        :return: Les poids des différents termes.\n",
    "        :rtype: dict[string : number]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def getWeightsForStem(self, stem):\n",
    "        \"\"\"\n",
    "        Permet de récupérer les poids du terme indiqué dans chaque document du corpus.\n",
    "        \n",
    "        :param stem: Le terme à considérer\n",
    "\n",
    "        :type stem: string\n",
    "        \n",
    "        :return: Les poids du terme dans chaque document.\n",
    "        :rtype: dict[int : number]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def getWeightsForQuery(self, query):\n",
    "        \"\"\"\n",
    "        Permet de récupérer les poids de chaque terme dans la requete indiqué.\n",
    "        \n",
    "        :param query: La requete à considérer\n",
    "\n",
    "        :type query: string\n",
    "        \n",
    "        :return: Les poids du terme dans la requete.\n",
    "        :rtype: dict[string : number]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def getNormDoc(self, docId):\n",
    "        \"\"\"\n",
    "        Permet de récupérer la norme d'un document vectorisé.\n",
    "        \n",
    "        :param docId: L'id du document à considérer\n",
    "\n",
    "        :type docId: int\n",
    "        \n",
    "        :return: La norme du document vectorisé\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        #Si on n'a jamais calculé la norme du document, on le fait puis on l'enregistre avant de la retourner\n",
    "        if(docId not in self.norm):\n",
    "            docWeights = self.getWeightsForDoc(docId)#On récupère le poids de chaque terme du document dans un dictionnaire\n",
    "            self.norm[docId] = np.linalg.norm(list(docWeights.values()))#On transforme en list/vecteur pour calculer la norme\n",
    "            #Remarque : la norme d'un vecteur à N dimensions est égale à la norme de ce vecteur auquel on rajoute M dimensions nulles. (||[1,2]|| = ||[0,0,1,2]||)\n",
    "        return self.norm[docId]\n",
    "    \n",
    "    def getNormQuery(self, query):\n",
    "        \"\"\"\n",
    "        Permet de récupérer la norme d'une requete vectorisé.\n",
    "        \n",
    "        :param query: Requete à considérer\n",
    "\n",
    "        :type query: string\n",
    "        \n",
    "        :return: La norme du document vectorisé\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        #On récupère le poids de chaque terme de la requete dans un dictionnaire\n",
    "        reqWeights = self.getWeightsForQuery(query)\n",
    "        #On transforme en list/vecteur pour calculer la norme\n",
    "        #Remarque : la norme d'un vecteur à N dimensions est égale à la norme de ce vecteur auquel on rajoute M dimensions nulles. (||[1,2]|| = ||[0,0,1,2]||)\n",
    "        return np.linalg.norm(list(reqWeights.values()))\n",
    "    \n",
    "    def getStemsFromQuery(query):\n",
    "        \"\"\"\n",
    "        Permet de récupérer l'ensemble des termes d'une requete.\n",
    "        \n",
    "        :param query: Requete à considérer\n",
    "\n",
    "        :type query: string\n",
    "        \n",
    "        :return: liste des termes de la requete\n",
    "        :rtype: list[string]\n",
    "        \"\"\"\n",
    "        return list(np.unique(list(map(p.stem, query.split())))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{t,d} = tf_{t,d}$ et $w_{t,q} = 1$ si $t \\in q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighter1(Weighter):\n",
    "    \"\"\"\n",
    "    CF class Weighter\n",
    "    \"\"\"\n",
    "    def getWeightsForDoc(self, idDoc):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        return self.index[idDoc] #correspond aux tfs des termes du document\n",
    "    \n",
    "    def getWeightsForStem(self, stem):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        return self.indexInverse[stem] if stem in self.indexInverse else {} #correspond aux tfs du terme pour chaque document\n",
    "    \n",
    "    def getWeightsForQuery(self, query):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        #récupère chaque stem de la requete et lui attribut une valeur de 1 car np.unique est utilisé\n",
    "        return dict(Counter(np.unique(list(map(p.stem, query.split())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{t,d} = tf_{t,d}$ et $w_{t,q} = tf_{t,q}$ si $t \\in q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighter2(Weighter):\n",
    "    \"\"\"\n",
    "    CF class Weighter\n",
    "    \"\"\"\n",
    "    def getWeightsForDoc(self, idDoc):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        return self.index[idDoc] #correspond aux tfs des termes du document\n",
    "    \n",
    "    def getWeightsForStem(self, stem):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        return self.indexInverse[stem] if stem in self.indexInverse else {} #correspond aux tfs du terme pour chaque document\n",
    "    \n",
    "    def getWeightsForQuery(self, query):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        #récupère chaque stem de la requete et lui attribut une valeur égale à son nombre\n",
    "        return dict(Counter(list(map(p.stem, query.split()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{t,d} = tf_{t,d}$ et $w_{t,q} = idf_t$ si $t \\in q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighter3(Weighter):\n",
    "    \"\"\"\n",
    "    CF class Weighter\n",
    "    \"\"\"\n",
    "    def getWeightsForDoc(self, idDoc):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        return self.index[idDoc] #correspond aux tfs des termes du document\n",
    "    \n",
    "    def getWeightsForStem(self, stem):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        return self.indexInverse[stem] if stem in self.indexInverse else {} #correspond aux tfs du terme pour chaque document\n",
    "    \n",
    "    def getWeightsForQuery(self, query):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        #On récupère chaque terme de manière unique\n",
    "        req=np.unique(list(map(p.stem, query.split())))\n",
    "        res={}\n",
    "        for stem in req:\n",
    "            res[stem] = self.getIdf(stem)#On récupère l'idf du terme considéré\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{t,d} = 1 + ln(tf_{t,d})$ et $w_{t,q} = idf_t$ si $t \\in q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighter4(Weighter):\n",
    "    \"\"\"\n",
    "    CF class Weighter\n",
    "    \"\"\"\n",
    "    def getWeightsForDoc(self, idDoc):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        #On associe à chaque terme le poids 1 + log(tf)\n",
    "        res={}\n",
    "        for stem in index[idDoc]:\n",
    "            res[stem] = 1+np.log(index[idDoc][stem])\n",
    "        return res\n",
    "    \n",
    "    def getWeightsForStem(self, stem):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        #On associe à chaque document le poids 1 + log(tf)\n",
    "        res={}\n",
    "        if stem in indexInverse:\n",
    "            for doc in indexInverse[stem]:\n",
    "                res[doc] = 1+np.log(indexInverse[stem][doc])\n",
    "        return res\n",
    "    \n",
    "    def getWeightsForQuery(self, query):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        #On récupère chaque terme de manière unique\n",
    "        req=np.unique(list(map(p.stem, query.split())))\n",
    "        res={}\n",
    "        for stem in req:\n",
    "            res[stem] = self.getIdf(stem)#On récupère l'idf du terme considéré\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{t,d} = (1 + ln(tf_{t,d})) \\times idf_t$ et $w_{t,q} = (1 + ln(tf_{t,q})) \\times idf_t$ si $t \\in q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighter5(Weighter):\n",
    "    \"\"\"\n",
    "    CF class Weighter\n",
    "    \"\"\"\n",
    "    def getWeightsForDoc(self, idDoc):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        #On associe à chaque terme le poids 1 + log(tf) * idf\n",
    "        res = {}\n",
    "        for stem in index[idDoc]:\n",
    "            idf = self.getIdf(stem)\n",
    "            res[stem] = (1+np.log(index[idDoc][stem])) * idf\n",
    "        return res\n",
    "    \n",
    "    def getWeightsForStem(self, stem):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        #On associe à chaque document le poids 1 + log(tf) * idf\n",
    "        res={}\n",
    "        if stem in indexInverse:\n",
    "            idf = self.getIdf(stem)\n",
    "            for doc in indexInverse[stem]:\n",
    "                res[doc] = (1+np.log(indexInverse[stem][doc])) * idf\n",
    "        return res\n",
    "    \n",
    "    def getWeightsForQuery(self, query):\n",
    "        \"\"\"\n",
    "        CF class Weighter\n",
    "        \"\"\"\n",
    "        #On associe à chaque terme le poids 1 + log(tf) * idf\n",
    "        tfs=dict(Counter(list(map(p.stem, query.split())))) #On calcul le tf pour les termes de la requete\n",
    "        res={}\n",
    "        for stem in tfs:\n",
    "            idf = self.getIdf(stem)\n",
    "            res[stem] = (1+np.log(tfs[stem]))*idf\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRModel():\n",
    "    \"\"\"\n",
    "    Squelette de classe permettant d'utiliser un modèle de Recherche d'Information.\n",
    "    \"\"\"\n",
    "    def __init__(self, weighter):\n",
    "        \"\"\"\n",
    "        Permet d'initialiser en indiquant le systeme de pondération utilisé\n",
    "        \n",
    "        :param weighter: Instance d'une classe Weighter\n",
    "\n",
    "        :type weighter: Weighter\n",
    "        \"\"\"\n",
    "        self.weighter = weighter\n",
    "    \n",
    "    def getScores(query):\n",
    "        \"\"\"\n",
    "        Permet de retourner le score de chaque document selon le modèle choisi.\n",
    "        \n",
    "        :param query: La requete à considérer\n",
    "\n",
    "        :type query: string\n",
    "        \n",
    "        :return: Le score de chaque document\n",
    "        :rtype: dict[int: number]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def getRanking(query):\n",
    "        \"\"\"\n",
    "        Permet de retourner un classement de document par pertinence selon le modèle choisi.\n",
    "        \n",
    "        :param query: L'id du document à considérer\n",
    "\n",
    "        :type query: string\n",
    "        \n",
    "        :return: liste des identifiants des documents du corpus triés par ordre décroissant de pertinence\n",
    "        :rtype: list[int]\n",
    "        \"\"\"\n",
    "        #On transforme le dictionnaire key:valeur en list de couple (key, valeur)\n",
    "        #On trie ensuite selon la valeur par ordre décroissant\n",
    "        dictToList = list(zip(query.keys(),query.values())) # Transforme un dict[key:val] en une list[(key,val)]\n",
    "        sortedList = sorted(dictToList, key=lambda e : e[1], reverse=True)#Permet de trier la liste selon les valeur\n",
    "        return np.array(sortedList)[:,0]#Permet de récupérer seulement les identifiants des dictionnaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarque :\n",
    "\n",
    "1) Le poids d'un terme n'appartenant pas à la requete sera toujours nul. Ainsi, le produit scalaire entre le vecteur de la requete et un vecteur de document ne prendra pas en compte les termes ne se trouvant pas dans la requete (multiplication par 0).<br/>\n",
    "\n",
    "Ainsi, on ne retournera pas les documents ayant un score nul (rapidité d'execution). La norme de chaque vecteur sera calculée la première fois que cela est nécessaire et sera gardée en mémoire pour la suite.<br/><br/>\n",
    "\n",
    "2) Le cosinus ($= \\frac{A \\cdot B}{||A|| \\times ||B||}$) sera toujours négatifcar les normes le sont et le produit scalaire aussi.<br/>\n",
    "En effet, les vecteurs auront toujours des poids positifs. La somme des produits de poids positifs sera aussi positif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectoriel(IRModel):\n",
    "    \"\"\"\n",
    "    Permet de représenter le modèle vectoriel en RI.\n",
    "    \"\"\"\n",
    "    def __init__(self, weighter, normalized = True):\n",
    "        \"\"\"\n",
    "        Permet d'initialisé en indiquant le mode de calcul utilisé entre le produit scalaire (normalized = False) et le cosinus (normalized = True)\n",
    "        \n",
    "        :param normalized: Booleen indiquant le mode de calcul à effectuer\n",
    "\n",
    "        :type normalized: boolean\n",
    "        \"\"\"\n",
    "        super().__init__(weighter)\n",
    "        self.normalized = normalized\n",
    "        \n",
    "    def getScores(self, query):\n",
    "        \"\"\"\n",
    "        CF class IRModel\n",
    "        \"\"\"\n",
    "        if(self.normalized == True):#calcul du cosinus\n",
    "            return self.getScoresNormalized(query)\n",
    "        else:#calcul du produit scalaire\n",
    "            return self.getScoresNotNormalized(query)\n",
    "        \n",
    "    def getScoresNormalized(self, query):\n",
    "        \"\"\"\n",
    "        Permet de récupérer le score en utilisant le cosinus entre les représentations vectorielles\n",
    "        \n",
    "        param query: La requete à considérer\n",
    "\n",
    "        :type query: string\n",
    "        \n",
    "        :return: Le score de chaque document\n",
    "        :rtype: dict[int: number]\n",
    "        \"\"\"\n",
    "        prodScalaires = self.getScoresNotNormalized(query)#Permet de récupérer les produits scalaires avec les différents documents\n",
    "        res={}\n",
    "        for docId,prod in prodScalaires.items():#Pour chaque produit scalaire, on divise par le produit des norm\n",
    "            res[docId] = prod/(self.weighter.getNormDoc(docId)*self.weighter.getNormQuery(query))\n",
    "        return res\n",
    "    \n",
    "    def getScoresNotNormalized(self, query):\n",
    "        \"\"\"\n",
    "        Permet de récupérer le score en utilisant le produit scalaire entre les représentations vectorielles des documents\n",
    "        \n",
    "        param query: La requete à considérer\n",
    "\n",
    "        :type query: string\n",
    "        \n",
    "        :return: Le score de chaque document\n",
    "        :rtype: dict[int: number]\n",
    "        \"\"\"\n",
    "        reqWeights = self.weighter.getWeightsForQuery(query) #On récupère les poids de la requetes\n",
    "        res={}\n",
    "        #Pour chaque terme de la requete (avec son poids associé)\n",
    "        for stem,weightStem in reqWeights.items():\n",
    "            docWeights = self.weighter.getWeightsForStem(stem)  #On récupère les poids du terme dans chaque document\n",
    "            #Pour chaque document contenant le terme courant (et son poids associé)\n",
    "            for docId,weightDoc in docWeights.items():\n",
    "                if(docId not in res):#Si le document n'a pas encore été rencontré\n",
    "                    res[docId] = weightStem*weightDoc#On ajoute le document avec comme valeur le produit des poids\n",
    "                else:\n",
    "                    res[docId] += weightStem*weightDoc#On ajoute le produit des poids à l'ancienne valeur\n",
    "        return res    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- TEST ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0.5773502691896258, 1: 0.26726124191242434}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=Weighter1(index, indexInverse)\n",
    "t=Vectoriel(w, True)\n",
    "t.getScores(\"rise encounter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 20), (1, 0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query={1:0,2:20}\n",
    "np.array(sorted(list(zip(query.keys(),query.values())), key=lambda e : e[0], reverse=False))[:,0]\n",
    "sorted(list(zip(query.keys(),query.values())), key=lambda e : e[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 30), (2, 20)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query={1:30,2:20}\n",
    "l=list(zip(query.keys(),query.values()))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sorted(list(zip(query.keys(),query.values())), key=lambda e : e[1], reverse=False))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 1  | v: 10\n",
      "k: 2  | v: 20\n",
      "k: 3  | v: 30\n"
     ]
    }
   ],
   "source": [
    "d={1:10,2:20,3:30}\n",
    "for (k,v) in d.items():\n",
    "    print(\"k:\",k,\" | v:\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.416573867739416"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(list(d.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 2, 4, 4, 6, 6, 8, 8, 10]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i if i%2==0 else i+1 for i in range(0,10) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.a = 10\n",
    "        \n",
    "class B(A):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.b = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=Weighter(index, indexInverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['home', 'sale', 'top']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weighter.getStemsFromQuery(\"homes sales top\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèles de langues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Okapi(IRModel):\n",
    "    def __init__(self, weighter, k1=1.2, b=0.75):\n",
    "        \n",
    "        super().__init__(weighter)\n",
    "        # constantes du modèle\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        \n",
    "        #nombre de docs\n",
    "        self.nbDoc = len(weighter.index)\n",
    "        \n",
    "        #longueur moyenne des documents\n",
    "        self.avglen = np.mean([sum(list(index[idDoc].values())) for idDoc in range(len(docs))])\n",
    "        \n",
    "    def getScores(self, query):\n",
    "        res={}\n",
    "        #Récupère les termes de la requete\n",
    "        stems = Weighter.getStemsFromQuery(query)\n",
    "        #Pour chaque terme de la requete\n",
    "        for stem in stems:\n",
    "            #Calcul des éléments indépendant du document\n",
    "            # on considère le dénominateur en 2 parties indépendantes du document (il faut développer le dénominateur):\n",
    "            # 1) k1 * (1 - b)\n",
    "            # 2) k1 * b / avgdl \n",
    "            idf = self.weighter.getIdf(stem)\n",
    "            denom1 = self.k1 * (1-self.b) \n",
    "            denom2 = self.k1 * self.b / self.avglen #parie du dénomianteur que l'on multipliera par la longueur du doc\n",
    "            \n",
    "            docWeights = self.weighter.getWeightsForStem(stem)  #On récupère les poids du terme dans chaque document\n",
    "            for idDoc, weight in docWeights.items():\n",
    "                lenDoc = sum(self.weighter.index[idDoc].values()) #taille doc = nombre de terme avec doublon\n",
    "                #lenDoc = len(self.weighter.index[idDoc]) #taille du doc = nombre de terme différent\n",
    "                score = idf * weight / (weight + denom1 + denom2*lenDoc) #Formule okapi-BM25\n",
    "                if(idDoc not in res):\n",
    "                    res[idDoc] = score\n",
    "                else:\n",
    "                    res[idDoc] += score\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Weighter2(index, indexInverse)\n",
    "o = Okapi(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = (\"new home forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5835791788863051, 1: 0.0, 2: 0.0, 3: 0.2238678032440597}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.getScores(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'new': 1,\n",
       "  'home': 1,\n",
       "  'ha': 1,\n",
       "  'been': 1,\n",
       "  'sale': 1,\n",
       "  'top': 1,\n",
       "  'forecast': 1},\n",
       " 1: {'home': 2, 'sale': 1, 'rise': 1, 'juli': 1},\n",
       " 2: {'is': 1, 'increas': 1, 'home': 1, 'sale': 1, 'juli': 1},\n",
       " 3: {'juli': 1, 'encount': 1, 'new': 1, 'home': 1, 'sale': 1, 'rise': 1}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
